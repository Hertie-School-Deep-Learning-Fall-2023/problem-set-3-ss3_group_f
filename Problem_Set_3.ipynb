{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLHcLz5zvGMH"
      },
      "source": [
        "# NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\n",
        "\n",
        "In this project we will be teaching a neural network to translate from German to English.\n",
        "\n",
        "\n",
        "This is made possible by the simple but powerful idea of the [sequence\n",
        "to sequence network](https://arxiv.org/abs/1409.3215), in which two\n",
        "recurrent neural networks work together to transform one sequence to\n",
        "another. An encoder network condenses an input sequence into a vector,\n",
        "and a decoder network unfolds that vector into a new sequence.\n",
        "\n",
        "\n",
        "To improve upon this model we'll use an [attention\n",
        "mechanism](https://arxiv.org/abs/1409.0473), which lets the decoder\n",
        "learn to focus over a specific range of the input sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs-8uyBGvGMP"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttoEcLnfvGMR"
      },
      "outputs": [],
      "source": [
        "# using Python 3.9\n",
        "%pip install pandas torch matplotlib numpy ipython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i381hyrevGMT"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import os\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.display import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULBf_g5pvGMV"
      },
      "source": [
        "## Loading & preprocessing training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3FBrubJvGMd"
      },
      "source": [
        "The files are all in Unicode, to simplify we will turn Unicode\n",
        "characters to ASCII, make everything lowercase, and trim most\n",
        "punctuation. We then implement a parsing funciton that reads a csv file from the specified path, normalizes every sentence in the dataset using the normalize function, and returns a list of (german_sentence, english_sentence) pairs. If the parameter reverse is True, the order of languages in the tuple are reversed resulting in reverse translation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q_GJNRPYvGMg"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalize_string(s):\n",
        "    s = unicode_to_ascii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def parse_data(path, reverse=False):\n",
        "\n",
        "  pairs = pd.read_csv(path)\n",
        "  pairs['GER'] = pairs['GER'].apply(normalize_string)\n",
        "  pairs['ENG'] = pairs['ENG'].apply(normalize_string)\n",
        "\n",
        "  if reverse:\n",
        "    pairs = pairs[['ENG', 'GER']]\n",
        "\n",
        "  return pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eHoLTeevGMk"
      },
      "source": [
        "Since there are a *lot* of example sentences and we want to train\n",
        "something quickly, we'll trim the data set to only relatively short and\n",
        "simple sentences and we will avoid explicit questions. Hence we will set a maximum sentence length of 10 words (that includes punctuation) and remove questions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sSAya9DovGMk"
      },
      "outputs": [],
      "source": [
        "MAX_WORDS = 10\n",
        "\n",
        "interrogative_words = ( \"what\", \"why\", \"how\", \"which\", \"where\", \"when\" , \"who\", \"whose\",\n",
        "                        \"was\", \"warum\", \"wie\", \"welche\", \"wo\", \"wann\", \"wer\", \"wieso\")\n",
        "\n",
        "\"\"\"\n",
        "TODO: Task 1 (10 pt)\n",
        "\n",
        "Implement the filter_pairs function so that it takes in a pd.DataFrame of language pairs and outputs another dataframe where following pairs are removed:\n",
        "\n",
        "- those where one of both exceeds the maximal sentence length (MAX_WORDS)\n",
        "- those where the pair contains any of the words defined in interrogative_words or a question mark.\n",
        "\n",
        "Note: your method should NOT employ any explicit iteration (i.e. for loops).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def filter_pairs(pairs,\n",
        "                 drop_words=interrogative_words,\n",
        "                 max_words=MAX_WORDS):\n",
        "  pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ou8g3IGvGMX"
      },
      "source": [
        "Each word in a language will be represented as a one-hot\n",
        "vector, or giant vector of zeros except for a single one (at the index\n",
        "of the word). Compared to the dozens of characters that might exist in a\n",
        "language, there are many many more words, so the encoding vector is much\n",
        "larger. We will however cheat a bit and trim the data to only use a few\n",
        "thousand words per language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfCoiIMHvGMY"
      },
      "source": [
        "We'll need a unique index per word to use as the inputs and targets of\n",
        "the networks later. To keep track of all this we will use a helper class\n",
        "called ``Language`` which has word → index (``word2index``) and index → word\n",
        "(``index2word``) dictionaries, as well as a count of each word\n",
        "``word2count`` which will be used to replace rare words later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yfuEobYvvGMb"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\"\"\"\n",
        "TODO: Task 2 (10 pt)\n",
        "\n",
        "Implement the function remove_rare_words, that removes words whose frequency is below\n",
        "min_freq and makes sure that the size of the vocabulary is below max_vocab_size\n",
        "by iteratively removing least frequent words. \n",
        "\n",
        "The function should also update the indices in the dictionary, so that no index \n",
        "is left unused (i.e. all values from 0 to n_words are indices) and all variables are up-to-date. \n",
        "\n",
        "You might want to define a helper method remove_word to break down the task.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Language:\n",
        "    def __init__(self):\n",
        "        self.word2index = {} # maps word to integer index\n",
        "        self.word2count = {} # maps word to its frequency\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"} # maps index to a word\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "\n",
        "    def remove_word(self, word):\n",
        "      pass\n",
        "\n",
        "    def remove_rare_words(self, min_freq, max_vocab_size):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGhDql9vGMl"
      },
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "-  Read text file and split into lines, split lines into pairs\n",
        "-  Normalize text, filter by length and content\n",
        "-  Make word lists from sentences in pairs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O5LZIJ9yvGMl"
      },
      "outputs": [],
      "source": [
        "def prepare_data(pairs, reverse=False):\n",
        "\n",
        "    print(f\"Read {len(pairs)} sentence pairs\")\n",
        "    pairs = filter_pairs(pairs)\n",
        "    print(f\"Filtered {len(pairs)} sentence pairs\")\n",
        "    pairs = pairs.to_numpy()\n",
        "\n",
        "    input_lang = Language()\n",
        "    output_lang = Language()\n",
        "\n",
        "    for pair in pairs:\n",
        "        input_lang.add_sentence(pair[0])\n",
        "        output_lang.add_sentence(pair[1])\n",
        "\n",
        "    print(f\"Input language: {input_lang.n_words} words\")\n",
        "    print(f\"Output language: {output_lang.n_words} words\")\n",
        "    return input_lang, output_lang, pairs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = os.path.join(os.getcwd(), \"data\", \"translations.csv\")\n",
        "pairs = parse_data(path)\n",
        "input_lang, output_lang, pairs = prepare_data(pairs, reverse=True)\n",
        "### SHOW NOTEBOOK OUTPUT ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWT9l3A0vGMl"
      },
      "source": [
        "## Encoder-decoders\n",
        "\n",
        "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
        "sequence and uses its own output as input for subsequent steps.\n",
        "\n",
        "A [Sequence to Sequence network](https://arxiv.org/abs/1409.3215)_, or\n",
        "seq2seq network, or [Encoder Decoder\n",
        "network](https://arxiv.org/pdf/1406.1078v3.pdf)_, is a model\n",
        "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
        "an input sequence and outputs a single vector, and the decoder reads\n",
        "that vector to produce an output sequence.\n",
        "\n",
        "Unlike sequence prediction with a single RNN, where every input\n",
        "corresponds to an output, the seq2seq model frees us from sequence\n",
        "length and order, which makes it ideal for translation between two\n",
        "languages, because there usally is not a 1:1 mapping between words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQdvPf6cvGMn"
      },
      "source": [
        "### The Encoder\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo1C0Gw8vGMn"
      },
      "outputs": [],
      "source": [
        "Image(filename=\"img/encoder.png\", height=400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UKk5mcDYvGMn"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, rnn_type='GRU', batch_size=1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batch_size\n",
        "        self.rnn_type = rnn_type\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size) #produces a list of vect\n",
        "        self.rnn = getattr(nn, rnn_type)(self.hidden_size, self.hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1) # translates input (series of indices for the respective words) into their vector representation\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, self.batch_size, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMmcFTutVDVM"
      },
      "source": [
        "Although all encoders must have certain distinctive features to be called as such, as for example the ability to process sequences of different length returning fixed-length hidden states, it is not required for them to have a recurrent structure. In fact, most state-of-the art NLP models use transformer layers, combining attention layers with feed-forward network for a higher parallelizability. Because transformer layers process information in parallel, they have no inherent mechanisms that makes them aware of the position of the word in the sentence and need to engineer a feature that encodes that information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SbA-XuPSN3QQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "TODO: Task 3 (10 pt)\n",
        "\n",
        "Implement a positional encoding module for an alternative encoder using transformers.\n",
        "The positional encoder should create a vector of size max_length containing the\n",
        "discrete values of a function f(x) applied on the even features of the input\n",
        "and g(x) applied on the odd features of the input.\n",
        "f(x) should be defaulted to sine, g(x) to cosine.\n",
        "Their value should be scaled by div_term.\n",
        "\n",
        "Briefly describe (3-4 sentences):\n",
        "\n",
        "- why sinusoidal are used by default\n",
        "- how div_term impacts the encoding\n",
        "- which characteristics would make a function a suitable\n",
        "  alternative to sinusoidals.\n",
        "\n",
        "Note: you are not expected to train the TransformerEncoder (this will most likely\n",
        "not work due to incompatibility with the train method). Simply show you can initialize\n",
        "the encoder and pass a tensor through it.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 hidden_size,\n",
        "                 max_length,\n",
        "                 even_fc=torch.sin,\n",
        "                 odd_fc=torch.cos):\n",
        "\n",
        "        super().__init__()\n",
        "        div_term = 1 / (torch.exp(torch.arange(0, hidden_size, 2, dtype=torch.float) *\n",
        "                             (math.log(10000.0) / hidden_size)))\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"In the forward step, the positional_encoder vector should be added to the input.\"\"\"\n",
        "        pass\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, max_length=MAX_WORDS, batch_size=1):\n",
        "\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.positional_encoding = PositionalEncoding(hidden_size, max_length)\n",
        "        self.transformer_layers = nn.TransformerEncoderLayer(hidden_size, nhead=4)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layers, num_layers=6)\n",
        "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, positional_encoding=False):\n",
        "    \n",
        "        \n",
        "        embedded = self.embedding(input) # translates input (series of indices for the respective words) into their vector representation\n",
        "        \n",
        "        # If true, applies positional encoding\n",
        "        if positional_encoding:\n",
        "            embedded = self.positional_encoding(embedded).view(1, 1, -1)\n",
        "\n",
        "        # Transformer encoder layers\n",
        "        transformer_output = self.transformer_encoder(embedded)\n",
        "\n",
        "        # Fully connected layer\n",
        "        output = self.fc(transformer_output)\n",
        "\n",
        "        return output\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "attn_encoder = TransformerEncoder(input_lang.n_words, 256)\n",
        "input = tensorFromSentence(input_lang, pairs[0][0])\n",
        "attn_encoder(input)\n",
        "### SHOW NOTEBOOK OUTPUT ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUCEEzEsvGMn"
      },
      "source": [
        "### The Decoder\n",
        "\n",
        "The decoder is another RNN that takes the encoder output vector(s) and\n",
        "outputs a sequence of words to create the translation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Und7tPLIvGMn"
      },
      "source": [
        "#### Attention Decoder\n",
        "\n",
        "\n",
        "Attention allows the decoder network to \"focus\" on a different part of the encoder's outputs for every step of the decoder's own outputs.\n",
        "For this, we determine a set of attention weights which capture which part of the sentence is most important for the translation.\n",
        "The attention weights will be multiplied by the encoder output vectors to create a weighted combination.\n",
        "The result should contain information about that specific part of the input sequence, and thus help the decoder choose the right output words.\n",
        "\n",
        "Specifically, this is done as follows:\n",
        "1. Attention weights are calculated with a feed-forward layer (`self.attn`) using the concatenated decoder's input and hidden state as inputs and appying a softmax to it.\n",
        "   * Because there are sentences of all sizes in the training data, to actually create and train this layer we have to choose a maximum sentence length (input length, for encoder outputs) that it can apply to. Sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few.\n",
        "1. The attention weights will be multiplied by the encoder output vectors to create a weighted combination (using `torch.bmm()`).\n",
        "1. The encoder outputs weighted by the attention weights are concatenated (use `torch.cat`) with the decoder's embeddings and fed into another feed-forward layer (`self.attn_combine`).\n",
        "1. ReLu is applied on the output of the attention module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PwdF09yvGMo"
      },
      "outputs": [],
      "source": [
        "Image(filename=\"img/decoder.png\", height=900)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7jQT5UPnvGMo"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "TODO: Task 4 (10 pt)\n",
        "Implement the attention layer as described above and depicted in the figure.\n",
        "\n",
        "Show the train progress (3-5 min) when using the decoder with attention on the translation task (see \"Pre-trained GRU seq2seq model\" section below).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, max_length):\n",
        "\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.max_length = max_length\n",
        "\n",
        "\n",
        "    def forward(self, embedded, hidden, encoder_outputs):\n",
        "        pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8c9yw5kE5oif"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, rnn_type='GRU', rnn_n_layers=1, batch_size=1, max_length=MAX_WORDS+1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.rnn_type = rnn_type\n",
        "        self.rnn_n_layers = rnn_n_layers\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.attention = Attention(self.hidden_size, self.max_length)\n",
        "        self.rnn = getattr(nn, self.rnn_type)(self.hidden_size, self.hidden_size, self.rnn_n_layers)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        c0 = self.init_hidden()\n",
        "        # h0 = self.init_hidden() # reuse hidden state of LSTM/GRU cell or reinitialize?\n",
        "\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            attn_hidden = hidden[0] # only hidden, not cell state\n",
        "        else:\n",
        "            attn_hidden = hidden # GRU has no cell state\n",
        "\n",
        "        output = self.attention(embedded, attn_hidden, encoder_outputs)\n",
        "        output, hidden = self.rnn(output, hidden)\n",
        "        output = self.out(output[0])\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        weights = torch.zeros(self.rnn_n_layers, self.batch_size, self.hidden_size, device=device)\n",
        "        return (weights, weights) if self.rnn_type == 'LSTM' else weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XTWdb-uvGMp"
      },
      "source": [
        "\n",
        "# Training\n",
        "\n",
        "### Preparing Training Data\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4bSmXByOvGMp"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8tTVtV7vGMq"
      },
      "source": [
        "### Training the Model\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track\n",
        "of every output and the latest hidden state. Then the decoder is given\n",
        "the `<SOS>` token as its first input, and the last hidden state of the\n",
        "encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as\n",
        "each next input, instead of using the decoder's guess as the next input.\n",
        "Using teacher forcing causes it to converge faster but [when the trained\n",
        "network is exploited, it may exhibit\n",
        "instability](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf).\n",
        "\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation -\n",
        "intuitively it has learned to represent the output grammar and can \"pick\n",
        "up\" the meaning once the teacher tells it the first few words, but it\n",
        "has not properly learned how to create the sentence from the translation\n",
        "in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "``teacher_forcing_ratio`` up to use more of it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cRXbgFQnvGMq"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_WORDS+1):\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-gYAkcAvGMq"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0ei32eYlvGMr"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai3KWafxvGMr"
      },
      "source": [
        "### Plotting results\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values\n",
        "``plot_losses`` saved while training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UdWdrxmVvGMt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjaE0v19vGMr"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XkR7FiesvGMr"
      },
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of7lko38vGMt"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "joGj-tUNvGMt"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_WORDS+1):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.init_hidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edCMUH-jvGMt"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "k2AM2KAOvGMu"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsYC2ekZvGMu"
      },
      "source": [
        "## Training and Evaluating\n",
        "\n",
        "With all these helper functions in place (it looks like extra work, but\n",
        "it makes it easier to run multiple experiments) we can actually\n",
        "initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small\n",
        "dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "single GRU layer. When training from scratch after about 40 minutes on a MacBook CPU we'll get some\n",
        "reasonable results.\n",
        "\n",
        "To speed-up the training, we do not randomly initialize the weights, but reuse the weights from a pretrained model.\n",
        "\n",
        "Note: run this cells to show that your code is correctly functioning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5-EsJsDZ6AWz"
      },
      "outputs": [],
      "source": [
        "hidden_size = 256\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size, rnn_type='GRU').to(device)\n",
        "attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, rnn_type='GRU', batch_size=1, dropout_p=0.1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvtS8nxr6CTH"
      },
      "outputs": [],
      "source": [
        "trainIters(encoder, attn_decoder, 10**3, print_every=100)\n",
        "### SHOW NOTEBOOK OUTPUT ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluateRandomly(encoder, attn_decoder)\n",
        "### SHOW NOTEBOOK OUTPUT ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96cuplb1vGMu"
      },
      "source": [
        "#### Pre-trained GRU seq2seq model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg8dG7-VvGMu"
      },
      "source": [
        "Load the state dict including the weights of a pre-trained model. If the initialization does not work, the architecture of your decoder is likely incorrect. Keep in mind that the two feed-forward layers of the Attention module have to be named `attn` and `attn_combine`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D4sQ1JhvGMu"
      },
      "outputs": [],
      "source": [
        "hidden_size = 256\n",
        "encoder_file = os.path.join('data', 'pretrained_encoder2.pt')\n",
        "pretrained_encoder = EncoderRNN(input_lang.n_words, hidden_size, rnn_type='GRU').to(device)\n",
        "pretrained_encoder.load_state_dict(torch.load(encoder_file))\n",
        "pretrained_encoder.eval()\n",
        "### SHOW NOTEBOOK OUTPUT ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQu7awkavGMu"
      },
      "outputs": [],
      "source": [
        "hidden_size = 256\n",
        "decoder_file = os.path.join('data', 'pretrained_decoder2.pt')\n",
        "pretrained_attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, rnn_type='GRU', dropout_p=0.1).to(device)\n",
        "pretrained_attn_decoder.load_state_dict(torch.load(decoder_file))\n",
        "pretrained_attn_decoder.eval()\n",
        "### SHOW NOTEBOOK OUTPUT ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyyCuL27vGMv"
      },
      "outputs": [],
      "source": [
        "trainIters(pretrained_encoder, pretrained_attn_decoder, 10**4, print_every=100)\n",
        "### SHOW NOTEBOOK OUTPUT ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iQtHR5YvGMv"
      },
      "outputs": [],
      "source": [
        "evaluateRandomly(pretrained_encoder, pretrained_attn_decoder)\n",
        "### SHOW NOTEBOOK OUTPUT ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZlnoQBPvGMy"
      },
      "source": [
        "# Transfer learning\n",
        "## Fine tune out-of-the box encoder-decoder model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C9JKpsPvGMz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "TODO: Task 5 (10 pt)\n",
        "\n",
        "Load a transformer for the hugging face library and fine-tune it to your problem.\n",
        "Show the training progress of the model and the training metrics. Briefly\n",
        "explain (3-4 sentences) the model choice and comment on the outcomes.\n",
        "\n",
        "Note: you don't have to use the above-defined methods for training.\n",
        "Splitting the dataset in train and test is welcomed, but not required for the task.\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEXE_Wa8vGMM"
      },
      "source": [
        "# Credits\n",
        "\n",
        "This problem set is based upon an official PyTorch [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html). Many thanks to PyTorch, [Sean Robertson](https://github.com/spro/practical-pytorch) and  [Florian Nachtigall](https://github.com/FlorianNachtigall).\n",
        "\n",
        "Be cautious with looking in the original notebook for answers. Many details have been changed and you won't be able to copy-and-paste solutions.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "6718ee69a5bb56082d0947ea42dc2c4709b07299f2a8d77179593d74021d8684"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
